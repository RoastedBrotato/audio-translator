version: '3.8'

services:
  asr_streaming:
    build:
      context: ./services/asr_streaming
      dockerfile: Dockerfile
    image: asr_streaming:latest
    ports:
      - "8003:8003"
    volumes:
      # Persist model caches so they don't re-download
      - whisper_cache:/root/.cache/whisper
      - torch_cache:/root/.cache/torch
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - HF_TOKEN=${HF_TOKEN}
      - SPEAKER_PROFILE_STORE_URL=${SPEAKER_PROFILE_STORE_URL:-http://localhost:8080}
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8003/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  translate_py:
    build:
      context: ./services/translate_py
      dockerfile: Dockerfile
    image: translate_py:latest
    ports:
      - "8004:8004"
    volumes:
      # Share HuggingFace cache with ASR service
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8004/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  tts_py:
    build:
      context: ./services/tts_py
      dockerfile: Dockerfile
    image: tts_py:latest
    ports:
      - "8005:8005"
    volumes:
      - tts_cache:/root/.local/share/tts
    environment:
      - COQUI_TOS_AGREED=1
      - CUDA_VISIBLE_DEVICES=0
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8005/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: unless-stopped

  embedding_service:
    build:
      context: ./services/embedding_service
      dockerfile: Dockerfile
    image: embedding_service:latest
    ports:
      - "8006:8006"
    volumes:
      # Share HuggingFace cache with other services
      - huggingface_cache:/root/.cache/huggingface
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8006/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: unless-stopped

  llm_service:
    build:
      context: ./services/llm_service
      dockerfile: Dockerfile
    image: llm_service:latest
    ports:
      - "8007:8007"
    environment:
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://ollama:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.1:8b}
    depends_on:
      ollama:
        condition: service_healthy
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8007/health').read()"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 4G
        reservations:
          cpus: '2'
          memory: 2G
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped

  postgres:
    image: ankane/pgvector:latest
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_DB=${DB_NAME:-audio_translator}
      - POSTGRES_USER=${DB_USER:-audio_translator}
      - POSTGRES_PASSWORD=${DB_PASSWORD:-audio_translator_pass}
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./migrations:/docker-entrypoint-initdb.d/schema
      - ./scripts/postgres-init.sh:/docker-entrypoint-initdb.d/01-init-keycloak.sh
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER:-audio_translator}"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: unless-stopped

  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    command: start-dev
    ports:
      - "8180:8080"
    environment:
      - KC_DB=postgres
      - KC_DB_URL=jdbc:postgresql://postgres:5432/keycloak
      - KC_DB_USERNAME=${DB_USER:-audio_translator}
      - KC_DB_PASSWORD=${DB_PASSWORD:-audio_translator_pass}
      - KEYCLOAK_ADMIN=admin
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD:-admin123}
      - KC_HEALTH_ENABLED=true
      - KC_METRICS_ENABLED=true
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/localhost/8080"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: unless-stopped

  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_ROOT_PASSWORD=${MINIO_ROOT_PASSWORD:-minioadmin123}
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: unless-stopped

  minio_init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER:-minioadmin} ${MINIO_ROOT_PASSWORD:-minioadmin123};
      mc mb myminio/${MINIO_BUCKET:-audio-translator-files} --ignore-existing;
      mc anonymous set download myminio/${MINIO_BUCKET:-audio-translator-files};
      exit 0;
      "

# Named volumes for persistent model storage
volumes:
  whisper_cache:
  torch_cache:
  huggingface_cache:  # Shared between ASR and translation services
  tts_cache:
  postgres_data:
  minio_data:
  ollama_data:  # Ollama model storage
